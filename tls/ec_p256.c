/**
 *		Tempesta TLS
 *
 * Elliptic curve NIST secp256r1 (prime256v1) over GF(p) in short Weierstrass
 * form, y^2 = x^3 - 3*x + b.
 *
 * References:
 *
 * 1. SEC1 http://www.secg.org/index.php?action=secg,docs_secg
 *
 * 2. GECC = Guide to Elliptic Curve Cryptography - Hankerson, Menezes, Vanstone
 *
 * 3. FIPS 186-3 http://csrc.nist.gov/publications/fips/fips186-3/fips_186-3.pdf
 *
 * 4. RFC 8422 for the related TLS structures and constants
 *
 * 5. [Curve25519] http://cr.yp.to/ecdh/curve25519-20060209.pdf
 *
 * 6. CORON, Jean-S'ebastien. Resistance against differential power analysis
 *    for elliptic curve cryptosystems. In : Cryptographic Hardware and
 *    Embedded Systems. Springer Berlin Heidelberg, 1999. p. 292-302.
 *    <http://link.springer.com/chapter/10.1007/3-540-48059-5_25>
 *
 * 7. M.Hedabou, P.Pinel, L.Beneteau, "A comb method to render ECC resistant
 *    against Side Channel Attacks", 2004.
 *
 * 8. Jacobian coordinates for short Weierstrass curves,
 *    http://www.hyperelliptic.org/EFD/g1p/auto-shortw-jacobian.html
 *
 * 9. S.Gueron, V.Krasnov, "Fast prime field elliptic-curve cryptography with
 *    256-bit primes", 2014.
 *
 * 10. NIST: Mathematical routines for the NIST prime elliptic curves, 2010.
 *
 * Copyright (C) 2020 Tempesta Technologies, Inc.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation; either version 2 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License along
 * with this program; if not, write to the Free Software Foundation, Inc.,
 * 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.
 */
#include "lib/str.h"
#include "asn1.h"
#include "bignum_asm.h"
#include "ecp.h"
#include "mpool.h"

/*
 * We can not use too large windows (W and G_W): 1 additional bit in the window
 * doubles precomputation time for m * P or memory for m * G, but number of
 * iterations on D and G_D decreases much slowly.
 */
#define G_BITS		256
#define G_LIMBS		(G_BITS / BIL)
#define G_W		7			/* m * G window bits */
#define G_W_SZ		(1U << (G_W - 1))	/* m * G window size */
#define G_D		((G_BITS + G_W - 1) / G_W)
#define W		4			/* m * P window bits */
#define W_SZ		(1U << (W - 1))		/* m * P window size */
#define D		((G_BITS + W - 1) / W)

static const struct {
	unsigned long	secp256r1_p[G_LIMBS];
	unsigned long	secp256r1_b[G_LIMBS];
	unsigned long	secp256r1_n[G_LIMBS];
	unsigned long	secp256r1_gx[G_LIMBS];
	unsigned long	secp256r1_gy[G_LIMBS];
	unsigned long	secp256r1_gz[G_LIMBS];

	TlsMpi		P;
	TlsMpi		B;
	TlsMpi		N;
	TlsMpi		__align_placeholder;
	TlsEcpPoint	G;
} ____cacheline_aligned __attribute__((packed)) G = {
	/*
	 * Domain parameters for secp256r1 (prime256v1) - generalized Mersenne primes.
	 */
	.secp256r1_p = {
		0xffffffffffffffffUL, 0xffffffffUL, 0UL, 0xffffffff00000001UL
	},
	.secp256r1_b = {
		0x3bce3c3e27d2604bUL, 0x651d06b0cc53b0f6UL,
		0xb3ebbd55769886bcUL, 0x5ac635d8aa3a93e7UL
	},
	.secp256r1_n = {
		0xf3b9cac2fc632551UL, 0xbce6faada7179e84UL,
		0xffffffffffffffffUL, 0xffffffff00000000UL
	},
	.secp256r1_gx = {
		0xf4a13945d898c296UL, 0x77037d812deb33a0UL,
		0xf8bce6e563a440f2UL, 0x6b17d1f2e12c4247UL
	},
	.secp256r1_gy = {
		0xcbb6406837bf51f5UL, 0x2bce33576b315eceUL,
		0x8ee7eb4a7c0f9e16UL, 0x4fe342e2fe1a7f9bUL
	},
	.secp256r1_gz = {
		1, 0, 0, 0
	},
	.P = {
		.s	= 1,
		.used	= G_LIMBS,
		.limbs	= G_LIMBS,
		._off	= -6 * (short)(G_LIMBS * CIL)
	},
	.B = {
		.s	= 1,
		.used	= G_LIMBS,
		.limbs	= G_LIMBS,
		._off	= -5 * (short)(G_LIMBS * CIL) - (short)sizeof(TlsMpi)
	},
	.N = {
		.s	= 1,
		.used	= G_LIMBS,
		.limbs	= G_LIMBS,
		._off	= -4 * (short)(G_LIMBS * CIL) - 2 * (short)sizeof(TlsMpi)
	},
	.__align_placeholder = {},
	.G = {
		.X = {
			.s	= 1,
			.used	= G_LIMBS,
			.limbs	= G_LIMBS,
			._off	= -3 * (short)(G_LIMBS * CIL)
				  - 4 * (short)sizeof(TlsMpi)
		},
		.Y = {
			.s	= 1,
			.used	= G_LIMBS,
			.limbs	= G_LIMBS,
			._off	= -2 * (short)(G_LIMBS * CIL)
				  - 5 * (short)sizeof(TlsMpi)
		},
		.Z = {
			.s	= 1,
			.used	= 1,
			.limbs	= G_LIMBS,
			._off	= -1 * (short)(G_LIMBS * CIL)
				  - 6 * (short)sizeof(TlsMpi)
		}
	}
};

typedef struct {
	unsigned long	x[G_LIMBS];
	unsigned long	y[G_LIMBS];
} EcpXY;

typedef struct {
	TlsEcpPoint	p;
	unsigned long	x[G_LIMBS * 2];
	unsigned long	y[G_LIMBS * 2];
	unsigned long	z[G_LIMBS];
} TmpEcpPoint;

/* Static precomputed table for the group generator. */
static const EcpXY combT_G[G_D + 1][G_W_SZ] __page_aligned_data = {
	#include "ecp256_G.autogen.h" /* Generated by t/tgen_ec256.c */
};
static DEFINE_PER_CPU(EcpXY, combT[W_SZ]);
static DEFINE_PER_CPU(TmpEcpPoint, combT_tmp[W_SZ]);

/**
 * Safe conditional assignment X = Y if @assign is 1.
 *
 * This function avoids leaking any information about whether the assignment was
 * done or not (the above code may leak information through branch prediction
 * and/or memory access patterns analysis). Leaking information about the
 * respective sizes of X and Y is ok however.
 */
static void
ecp256_safe_cond_assign(TlsMpi *X, const TlsMpi *Y, unsigned char assign)
{
	static const unsigned short s_masks[2] = {0, 0xffff};
	static const unsigned long l_masks[2] = {0, 0xffffffffffffffffUL};

	unsigned long *x = MPI_P(X), *y = MPI_P(Y);
	unsigned short s_mask;
	unsigned long l_mask;

	BUG_ON(X->used > G_LIMBS || Y->used > G_LIMBS);
	BUG_ON(X->limbs < Y->used);

	s_mask = s_masks[assign];
	l_mask = l_masks[assign];

	X->s ^= (X->s ^ Y->s) & s_mask;
	X->used ^= (X->used ^ Y->used) & s_mask;

	x[0] ^= (x[0] ^ y[0]) & l_mask;
	x[1] ^= (x[1] ^ y[1]) & l_mask;
	x[2] ^= (x[2] ^ y[2]) & l_mask;
	x[3] ^= (x[3] ^ y[3]) & l_mask;
}

/*
 * Fast mod-p functions expect their argument to be in the 0..p^2 range.
 *
 * In order to guarantee that, we need to ensure that operands of
 * multiplication are in the 0..p range. So, after each operation we will
 * bring the result back to this range.
 *
 * The following macros are shortcuts for doing that.
 */

/*
 * Reduce a TlsMpi mod p in-place, to use after ttls_mpi_sub_mpi
 * N->s < 0 is a very fast test, which fails only if N is 0
 */
static inline void
MOD_SUB(TlsMpi *N)
{
	while ((N)->s < 0 && !ttls_mpi_eq_0(N))
		ttls_mpi_add_mpi(N, N, &G.P);
}

/*
 * Reduce a TlsMpi mod p in-place, to use after ttls_mpi_add_mpi().
 * We known P, N and the result are positive, so sub_abs is correct, and
 * a bit faster.
 */
static inline void
MOD_ADD(TlsMpi *N)
{
	while (ttls_mpi_cmp_mpi(N, &G.P) >= 0)
		ttls_mpi_sub_abs(N, N, &G.P);
}

/*
 * For curves in short Weierstrass form, we do all the internal operations in
 * Jacobian coordinates.
 *
 * For multiplication, we'll use a comb method with coutermeasueres against
 * SPA, hence timing attacks.
 *
 * TODO #1064: use P256 is Montgomery-friendly [9], so use the OpenSSL
 * optimization techniques for the prime modulus, see [9]
 * chapter "3 A Montgomery-friendly modulus".
 */

static void
ecp256_mul_mod(TlsMpi *X, const TlsMpi *A, const TlsMpi *B)
{
	BUG_ON(A->s < 0 || B->s < 0);
	BUG_ON(A->used != G_LIMBS || B->used != G_LIMBS);

	mpi_mul_x86_64_4(MPI_P(X), MPI_P(A), MPI_P(B));

	mpi_fixup_used(X, G_LIMBS * 2);
	X->s = A->s * B->s;

	ecp_mod_p256_x86_64(MPI_P(X), X->used);
	mpi_fixup_used(X, G_LIMBS);
}

static void
ecp256_sqr_mod(TlsMpi *X, const TlsMpi *A)
{
	BUG_ON(X->limbs < G_LIMBS);
	BUG_ON(A->limbs < G_LIMBS);
	BUG_ON(A->s < 0);
	BUG_ON(A->used > G_LIMBS);

	mpi_sqr_x86_64_4(MPI_P(X), MPI_P(A));

	mpi_fixup_used(X, G_LIMBS * 2);
	X->s = 1;

	ecp_mod_p256_x86_64(MPI_P(X), X->used);
	mpi_fixup_used(X, G_LIMBS);
}

/**
 * Modular inverse X = A^-1 mod N , based on binary extended Euclidean algorithm.
 * The fuction is optimized for always odd @N (GECC 2.22).
 *
 * For the secp256 the algorithms typically takes 160-190 iterations, i.e.
 * 640-760 right shifts and subs/adds.
 *
 * TODO #1064 see big_num_math-denis, chapter 9.
 * p256 [9]: use Little Fermat theorem and G.P specialization
 * OpenSSL [9]: 255 Montgomery squares (MSQR) and 13 multiplications (MM)
 * or 12 MMs if X coordinage only is needed.
 */
void
ecp256_inv_mod(TlsMpi *X, const TlsMpi *A, const TlsMpi *N)
{
	TlsMpi *U, *V, *X1, *X2;

	U = ttls_mpool_alloc_stack(sizeof(TlsMpi) * 4 + (G_LIMBS * 4 + 2) * CIL);
	V = ttls_mpi_init_next(U, G_LIMBS);
	X1 = ttls_mpi_init_next(V, G_LIMBS);
	X2 = ttls_mpi_init_next(X1, G_LIMBS + 1);
	ttls_mpi_init_next(X2, G_LIMBS + 1);

	ttls_mpi_copy(U, A);
	ttls_mpi_copy(V, N);
	ttls_mpi_lset(X1, 1);
	ttls_mpi_lset(X2, 0);

loop:
	// TODO #1064 the 4 shifts can be done in SIMD.
	while (!(MPI_P(U)[0] & 1)) {
		ttls_mpi_shift_r(U, 1);
		if ((MPI_P(X1)[0] & 1))
			ttls_mpi_add_mpi(X1, X1, N);
		ttls_mpi_shift_r(X1, 1);
	}

	while (!(MPI_P(V)[0] & 1)) {
		ttls_mpi_shift_r(V, 1);
		if ((MPI_P(X2)[0] & 1))
			ttls_mpi_add_mpi(X2, X2, N);
		ttls_mpi_shift_r(X2, 1);
	}

	if (ttls_mpi_cmp_mpi(U, V) >= 0) {
		ttls_mpi_sub_mpi(U, U, V);
		ttls_mpi_sub_mpi(X1, X1, X2);
	} else {
		ttls_mpi_sub_mpi(V, V, U);
		ttls_mpi_sub_mpi(X2, X2, X1);
	}

	if (ttls_mpi_eq_1(U)) {
		ttls_mpi_copy(X, X1);
	}
	else if (ttls_mpi_eq_1(V)) {
		ttls_mpi_copy(X, X2);
	}
	else {
		goto loop;
	}

	while (ttls_mpi_cmp_int(X, 0) < 0)
		ttls_mpi_add_mpi(X, X, N);
}

/*
 * Normalize jacobian coordinates so that Z == 0 || Z == 1  (GECC 3.2.1)
 * Cost: 1N := 1I + 3M + 1S
 */
static int
ecp256_normalize_jac(TlsEcpPoint *pt)
{
	TlsMpi *Zi, *ZZi;

	if (ttls_mpi_eq_0(&pt->Z))
		return 0;

	Zi = ttls_mpi_alloc_stack_init(G_LIMBS);
	ZZi = ttls_mpi_alloc_stack_init(G_LIMBS * 2);

	/* X = X / Z^2  mod p */
	ecp256_inv_mod(Zi, &pt->Z, &G.P);
	ecp256_sqr_mod(ZZi, Zi);
	ecp256_mul_mod(&pt->X, &pt->X, ZZi);

	/* Y = Y / Z^3  mod p */
	ecp256_mul_mod(&pt->Y, &pt->Y, ZZi);
	ecp256_mul_mod(&pt->Y, &pt->Y, Zi);

	/* Z = 1 */
	ttls_mpi_lset(&pt->Z, 1);

	return 0;
}

/**
 * @t_len is very small, log(W_SZ) = W - 1 in run time or log(G_W_SZ) = W_SZ -1
 * for the G points precomputation.
 */
static void
ecp256_normalize_jac_many(TlsEcpPoint *T[], size_t t_len)
{
#define __INIT_C(i)							\
do {									\
	c[i].limbs = G_LIMBS * 2;					\
	c[i]._off = (unsigned long)p_limbs - (unsigned long)(c + i);	\
	p_limbs += G_LIMBS * 2;						\
} while (0)

	int i;
	unsigned long *p_limbs;
	TlsMpi *u, *Zi, *ZZi, *c;

	/*
	 * TODO #1064 alloc const sized MPI array on the stack; alloc
	 * limbs only and throw out the MPI wrapper functions and descriptors.
	 */
	c = ttls_mpool_alloc_stack((sizeof(TlsMpi) + G_LIMBS * 2 * CIL) * t_len);
	u = ttls_mpi_alloc_stack_init(G_LIMBS * 2);
	Zi = ttls_mpi_alloc_stack_init(G_LIMBS * 2);
	ZZi = ttls_mpi_alloc_stack_init(G_LIMBS * 2);
	p_limbs = (unsigned long *)&c[t_len];

	/* c[i] = Z_0 * ... * Z_i */
	__INIT_C(0);
	ttls_mpi_copy_alloc(&c[0], &T[0]->Z, false);
	for (i = 1; i < t_len; i++) {
		__INIT_C(i);
		ecp256_mul_mod(&c[i], &c[i - 1], &T[i]->Z);
	}

	/* u = 1 / (Z_0 * ... * Z_n) mod P */
	ecp256_inv_mod(u, &c[t_len - 1], &G.P);

	for (i = t_len - 1; i >= 0; i--) {
		/*
		 * Zi = 1 / Z_i mod p
		 * u = 1 / (Z_0 * ... * Z_i) mod P
		 */
		if (!i) {
			ttls_mpi_copy(Zi, u);
		} else {
			ecp256_mul_mod(Zi, u, &c[i - 1]);
			ecp256_mul_mod(u, u, &T[i]->Z);
		}

		/* proceed as in normalize(). */
		ecp256_sqr_mod(ZZi, Zi);
		ecp256_mul_mod(&T[i]->X, &T[i]->X, ZZi);
		ecp256_mul_mod(&T[i]->Y, &T[i]->Y, ZZi);
		ecp256_mul_mod(&T[i]->Y, &T[i]->Y, Zi);
		/*
		 * At the moment Z coordinate stores a garbage, so free
		 * it now and treat as 1 on subsequent processing.
		 */
		ttls_mpi_lset(&T[i]->Z, 1);
	}

	ttls_mpi_pool_cleanup_ctx((unsigned long)c, false);
#undef __INIT_C
}

/**
 * Conditional point inversion: Q -> -Q = (Q.X, -Q.Y, Q.Z) without leak.
 * "inv" must be 0 (don't invert) or 1 (invert) or the result will be invalid.
 */
static void
ecp256_safe_invert_jac(TlsEcpPoint *Q, unsigned char inv)
{
	unsigned char nonzero;
	TlsMpi *mQY = ttls_mpi_alloc_stack_init(G_LIMBS);

	/* Use the fact that -Q.Y mod P = P - Q.Y unless Q.Y == 0 */
	ttls_mpi_sub_mpi(mQY, &G.P, &Q->Y);
	nonzero = !ttls_mpi_eq_0(&Q->Y);

	ecp256_safe_cond_assign(&Q->Y, mQY, inv & nonzero);
}

/**
 * Point doubling R = 2 P, Jacobian coordinates [8, "dbl-1998-cmo-2"].
 *
 * Cost: ( 3M + 3S if P->Z == 1 (rarely) and 4M + 4S otherwise)
 *	 + 7A + 5shift(*2)
 *
 * TODO #1064 the cost isn' the best one according to [8].
 *
 * TODO #1064 while the doubling should be much faster than addition, in our
 * case this isn't true since the function uses more heavy calls on MPI copying,
 * subtraction, shifts and so on.
 */
static int
ecp256_double_jac(TlsEcpPoint *R, const TlsEcpPoint *P)
{
	TlsMpi M, S, T, U;

	ttls_mpi_alloca_init(&M, G_LIMBS * 2);
	ttls_mpi_alloca_init(&S, G_LIMBS * 2);
	ttls_mpi_alloca_init(&T, G_LIMBS * 2);
	ttls_mpi_alloca_init(&U, G_LIMBS * 2);

	/* M = 3(X + Z^2)(X - Z^2) */
	if (likely(!ttls_mpi_eq_1(&P->Z)))
		ecp256_sqr_mod(&S, &P->Z);
	else
		ttls_mpi_lset(&S, 1);
	ttls_mpi_add_mpi(&T, &P->X, &S);
	MOD_ADD(&T);
	ttls_mpi_sub_mpi(&U, &P->X, &S);
	MOD_SUB(&U);
	ecp256_mul_mod(&S, &T, &U);
	ttls_mpi_copy_alloc(&M, &S, false);
	ttls_mpi_shift_l(&M, 1);
	ttls_mpi_add_mpi(&M, &M, &S);
	MOD_ADD(&M);

	/* S = 4 * X * Y^2 */
	ecp256_sqr_mod(&T, &P->Y);
	ttls_mpi_shift_l(&T, 1);
	MOD_ADD(&T);
	ecp256_mul_mod(&S, &P->X, &T);
	ttls_mpi_shift_l(&S, 1);
	MOD_ADD(&S);

	/* U = 8.Y^4 */
	ecp256_sqr_mod(&U, &T);
	ttls_mpi_shift_l(&U, 1);
	MOD_ADD(&U);

	/* T = M^2 - 2 * S */
	ecp256_sqr_mod(&T, &M);
	ttls_mpi_sub_mpi(&T, &T, &S);
	MOD_SUB(&T);
	ttls_mpi_sub_mpi(&T, &T, &S);
	MOD_SUB(&T);

	/* S = M(S - T) - U */
	ttls_mpi_sub_mpi(&S, &S, &T);
	MOD_SUB(&S);
	ecp256_mul_mod(&S, &S, &M);
	ttls_mpi_sub_mpi(&S, &S, &U);
	MOD_SUB(&S);

	/* U = 2 * Y * Z */
	if (likely(!ttls_mpi_eq_1(&P->Z)))
		ecp256_mul_mod(&U, &P->Y, &P->Z);
	else
		ttls_mpi_copy(&U, &P->Y);
	ttls_mpi_shift_l(&U, 1);
	MOD_ADD(&U);

	ttls_mpi_copy(&R->X, &T);
	ttls_mpi_copy(&R->Y, &S);
	ttls_mpi_copy(&R->Z, &U);

	return 0;
}

/*
 * Addition: R = P + Q, mixed affine-Jacobian coordinates (GECC 3.22)
 *
 * #TODO #1064: the implementation uses formula [8, "madd-2008-g"] and I'm not
 * sure if it's the most efficient one - [9] refernces another formula.
 * Explore "New Point Addition Formulae for ECCApplications" by Meloni 2007.
 *
 * The coordinates of Q must be normalized (= affine),
 * but those of P don't need to. R is not normalized.
 *
 * Special cases: (1) P or Q is zero, (2) R is zero, (3) P == Q.
 * None of these cases can happen as intermediate step in ecp_mul_comb():
 * - at each step, P, Q and R are multiples of the base point, the factor
 *   being less than its order, so none of them is zero;
 * - Q is an odd multiple of the base point, P an even multiple,
 *   due to the choice of precomputed points in the modified comb method.
 * So branches for these cases do not leak secret information.
 *
 * We accept Q->Z being unset (saving memory in tables) as meaning 1.
 *
 * Cost: 1A := 8M + 3S (same as Chudnovsky-Affine time, GECC 3.2.2).
 */
static int
ecp256_add_mixed(TlsEcpPoint *R, const TlsEcpPoint *P, const TlsEcpPoint *Q)
{
	TlsMpi T1, T2, T3, T4, X, Y, Z;

	/* Trivial cases: P == 0 or Q == 0 (case 1). */
	if (ttls_mpi_eq_0(&P->Z)) {
		ttls_ecp_copy(R, Q);
		return 0;
	}
	if (!ttls_mpi_empty(&Q->Z)) {
		if (ttls_mpi_eq_0(&Q->Z)) {
			ttls_ecp_copy(R, P);
			return 0;
		}
		/* Make sure Q coordinates are normalized. */
		if (!ttls_mpi_eq_1(&Q->Z))
			return -EINVAL;
	}

	ttls_mpi_alloca_init(&T1, G_LIMBS * 2);
	ttls_mpi_alloca_init(&T2, G_LIMBS * 2);
	ttls_mpi_alloca_init(&T3, G_LIMBS * 2);
	ttls_mpi_alloca_init(&T4, G_LIMBS * 2);
	ttls_mpi_alloca_init(&X, G_LIMBS * 2);
	ttls_mpi_alloca_init(&Y, G_LIMBS * 2);
	ttls_mpi_alloca_init(&Z, G_LIMBS * 2);

	if (unlikely(ttls_mpi_eq_1(&P->Z))) {
		/* Relatively rare case, ~1/60. */
		ttls_mpi_sub_mpi(&T1, &Q->X, &P->X);
		MOD_SUB(&T1);
		ttls_mpi_sub_mpi(&T2, &Q->Y, &P->Y);
		MOD_SUB(&T2);
	} else {
		ecp256_sqr_mod(&T1, &P->Z);
		ecp256_mul_mod(&T2, &T1, &P->Z);
		ecp256_mul_mod(&T1, &T1, &Q->X);
		ecp256_mul_mod(&T2, &T2, &Q->Y);
		ttls_mpi_sub_mpi(&T1, &T1, &P->X);
		MOD_SUB(&T1);
		ttls_mpi_sub_mpi(&T2, &T2, &P->Y);
		MOD_SUB(&T2);
	}

	/* Special cases (2) and (3) */
	if (ttls_mpi_eq_0(&T1)) {
		if (ttls_mpi_eq_0(&T2))
			return ecp256_double_jac(R, P);
		ttls_ecp_set_zero(R);
		return 0;
	}

	if (unlikely(ttls_mpi_eq_1(&P->Z)))
		ttls_mpi_copy_alloc(&Z, &T1, false);
	else
		ecp256_mul_mod(&Z, &P->Z, &T1);
	ecp256_sqr_mod(&T3, &T1);
	ecp256_mul_mod(&T4, &T3, &T1);
	ecp256_mul_mod(&T3, &T3, &P->X);
	ttls_mpi_copy_alloc(&T1, &T3, false);
	ttls_mpi_shift_l(&T1, 1);
	MOD_ADD(&T1);
	ecp256_sqr_mod(&X, &T2);
	ttls_mpi_sub_mpi(&X, &X, &T1);
	MOD_SUB(&X);
	ttls_mpi_sub_mpi(&X, &X, &T4);
	MOD_SUB(&X);
	ttls_mpi_sub_mpi(&T3, &T3, &X);
	MOD_SUB(&T3);
	ecp256_mul_mod(&T3, &T3, &T2);
	ecp256_mul_mod(&T4, &T4, &P->Y);
	ttls_mpi_sub_mpi(&Y, &T3, &T4);
	MOD_SUB(&Y);

	/* Resulting coorinates are twice smaller than the temporary MPIs. */
	ttls_mpi_copy(&R->X, &X);
	ttls_mpi_copy(&R->Y, &Y);
	ttls_mpi_copy(&R->Z, &Z);

	return 0;
}

/*
 * Recode the secret scalar `m` that will be used with our comb method.
 *
 * Basically, we use the same comb algorithm modification as mbed TLS.
 *
 * The basic comb method is described in GECC 3.44 for example. We use a
 * modified version that provides resistance to SPA by avoiding zero
 * digits in the representation as in [3]. We modify the method further by
 * requiring that all K_i be odd, which has the small cost that our
 * representation uses one more K_i, due to carries, but saves on the size of
 * the precomputed table.
 *
 * Summary of the comb method and its modifications:
 *
 * - The goal is to compute m*P for some w*d-bit integer m.
 *
 * - The basic comb method splits m into the w-bit integers
 *   x[0] .. x[d-1] where x[i] consists of the bits in m whose
 *   index has residue i modulo d, and computes m * P as
 *   S[x[0]] + 2 * S[x[1]] + .. + 2^(d-1) S[x[d-1]], where
 *   S[i_{w-1} .. i_0] := i_{w-1} 2^{(w-1)d} P + ... + i_1 2^d P + i_0 P.
 *
 * - If it happens that, say, x[i+1]=0 (=> S[x[i+1]]=0), one can replace the sum by
 *   .. + 2^{i-1} S[x[i-1]] - 2^i S[x[i]] + 2^{i+1} S[x[i]] + 2^{i+2} S[x[i+2]] ..,
 *   thereby successively converting it into a form where all summands
 *   are nonzero, at the cost of negative summands. This is the basic idea of [3].
 *
 * - More generally, even if x[i+1] != 0, we can first transform the sum as
 *   .. - 2^i S[x[i]] + 2^{i+1} ( S[x[i]] + S[x[i+1]] ) + 2^{i+2} S[x[i+2]] ..,
 *   and then replace S[x[i]] + S[x[i+1]] = S[x[i] ^ x[i+1]] + 2 S[x[i] & x[i+1]].
 *   Performing and iterating this procedure for those x[i] that are even
 *   (keeping track of carry), we can transform the original sum into one of the
 *   form S[x'[0]] +- 2 S[x'[1]] +- .. +- 2^{d-1} S[x'[d-1]] + 2^d S[x'[d]]
 *   with all x'[i] odd. It is therefore only necessary to know S at odd indices,
 *   which is why we are only computing half of it in the first place in
 *   ecp256_precompute_comb() and accessing it with index abs(i) / 2 in
 *   ecp256_select_comb().
 *
 * - For the sake of compactness, only the seven low-order bits of x[i]
 *   are used to represent its absolute value (K_i in the paper), and the msb
 *   of x[i] encodes the sign (s_i in the paper): it is set if and only if
 *   if s_i == -1;
 *
 * Calling conventions:
 * - x is an array of size d + 1
 * - w is the size, ie number of teeth, of the comb, and must be between
 *   2 and 7
 * - m is the MPI, expected to be odd and such that bitlength(m) <= w * d
 *   (the result will be incorrect if these assumptions are not satisfied)
 *
 * TODO #1064 fetch required precomputed points from T (GECC Note 3.46)
 * (sinle T-pass, multiple x passes, which is sizeof(d)=small.
 * Use scatter AVX2 instruction to load values from memory?
 */
static void
ecp256_comb_fixed(unsigned char x[], size_t d, unsigned char w, const TlsMpi *m)
{
	size_t i, j, b, bits = m->used * BIL;
	unsigned long *p = MPI_P(m);
	unsigned char c, cc, adjust;

	bzero_fast(x, d + 1);

	/* First get the classical comb values (except for x_d = 0) */
	for (i = 0; i < d; i++)
		for (j = 0; j < w; j++) {
			b = i + d * j;
			if (unlikely(b >= bits))
				break;
			x[i] |= ((p[b >> BSHIFT] >> (b & BMASK)) & 1) << j;
		}

	/* Now make sure x_1 .. x_d are odd */
	for (c = 0, i = 1; i <= d; i++) {
		/* Add carry and update it */
		cc = x[i] & c;
		x[i] = x[i] ^ c;
		c = cc;

		/* Adjust if needed, avoiding branches */
		adjust = 1 - (x[i] & 1);
		c |= x[i] & (x[i - 1] * adjust);
		x[i] = x[i] ^ (x[i - 1] * adjust);
		x[i - 1] |= adjust << 7;
	}
}

/*
 * Precompute points for the comb method
 *
 * If i = i_{w-1} ... i_1 is the binary representation of i, then
 * T[i] = i_{w-1} 2^{(w-1)d} P + ... + i_1 2^d P + P
 *
 * T must be able to hold 2^{w - 1} elements
 *
 * Cost: d(w-1) D + (2^{w-1} - 1) A + 1 N(w-1) + 1 N(2^{w-1} - 1)
 * For w=4,D=4S+4M,A=8M+3S,S=0.8M this gives about 1004*M + 2*I.
 */
static int
ecp256_precompute_comb(const unsigned long *pXY)
{
	int i, j, k;
	TlsEcpPoint *cur, *TT[W_SZ];
	TmpEcpPoint *T = *this_cpu_ptr(&combT_tmp);
	EcpXY *Txy = *this_cpu_ptr(&combT);

	if (unlikely(!T[0].p.X._off)) {
		for (i = 0; i < W_SZ; ++i) {
			T[i].p.X.limbs = G_LIMBS * 2;
			T[i].p.X._off = (long)T[i].x - (long)&T[i].p.X;
			T[i].p.Y.limbs = G_LIMBS * 2;
			T[i].p.Y._off = (long)T[i].y - (long)&T[i].p.Y;
			T[i].p.Z.limbs = G_LIMBS;
			T[i].p.Z._off = (long)T[i].z - (long)&T[i].p.Z;
		}
	}

	/*
	 * Set T[0] = P and T[2^{i-1}] = 2^{di} P for i = 1 .. w-1
	 * (this is not the final value).
	 */
	T->p.X.s = 1;
	memcpy_fast(T->x, pXY, G_LIMBS * CIL);
	mpi_fixup_used(&T->p.X, G_LIMBS);
	T->p.Y.s = 1;
	memcpy_fast(T->y, pXY + G_LIMBS, G_LIMBS * CIL);
	mpi_fixup_used(&T->p.Y, G_LIMBS);
	ttls_mpi_lset(&T->p.Z, 1);

	for (k = 0, i = 1; i < W_SZ; i <<= 1) {
		cur = &T[i].p;
		ttls_ecp_copy(cur, &T[i >> 1].p);
		for (j = 0; j < D; j++)
			/*
			 * TODO #1064 use repeated doubling optimization.
			 * E.g. see sp_256_proj_point_dbl_n_store_avx2_4() and
			 * sp_256_proj_point_dbl_n_avx2_4() from WolfSSL.
			 * See algorithm GECC 3.23.
			 */
			MPI_CHK(ecp256_double_jac(cur, cur));

		TT[k++] = cur;
	}
	ecp256_normalize_jac_many(TT, k);

	/*
	 * Compute the remaining ones using the minimal number of additions
	 * Be careful to update T[2^l] only after using it!
	 */
	for (k = 0, i = 1; i < W_SZ; i <<= 1) {
		j = i;
		while (j--) {
			MPI_CHK(ecp256_add_mixed(&T[i + j].p, &T[j].p, &T[i].p));
			TT[k++] = &T[i + j].p;
		}
	}
	ecp256_normalize_jac_many(TT, k);

	for (i = 0; i <= k; i++) {
		memcpy_fast(Txy[i].x, MPI_P(&T[i].p.X), G_LIMBS * CIL);
		memcpy_fast(Txy[i].y, MPI_P(&T[i].p.Y), G_LIMBS * CIL);
	}

	return 0;
}

/*
 * Select precomputed point: R = sign(i) * T[ abs(i) / 2 ]
 */
static void
ecp256_select_comb(TlsEcpPoint *R, const EcpXY T[], unsigned char t_len,
		   unsigned char i)
{
	static const unsigned long l_masks[2] = {0, 0xffffffffffffffffUL};
	unsigned long *rx = MPI_P(&R->X), *ry = MPI_P(&R->Y);
	unsigned char ii, j;

	R->X.s = 1;
	R->X.used = G_LIMBS;
	R->Y.s = 1;
	R->Y.used = G_LIMBS;

	/* Ignore the "sign" bit and scale down */
	ii =  (i & 0x7Fu) >> 1;

	/* Read the whole table to thwart cache-based timing attacks */
	for (j = 0; j < t_len; j++) {
		const unsigned long mask = l_masks[j == ii];

		rx[0] ^= (rx[0] ^ T[j].x[0]) & mask;
		rx[1] ^= (rx[1] ^ T[j].x[1]) & mask;
		rx[2] ^= (rx[2] ^ T[j].x[2]) & mask;
		rx[3] ^= (rx[3] ^ T[j].x[3]) & mask;

		ry[0] ^= (ry[0] ^ T[j].y[0]) & mask;
		ry[1] ^= (ry[1] ^ T[j].y[1]) & mask;
		ry[2] ^= (ry[2] ^ T[j].y[2]) & mask;
		ry[3] ^= (ry[3] ^ T[j].y[3]) & mask;
	}

	/* Safely invert result if i is "negative" */
	ecp256_safe_invert_jac(R, i >> 7);
}

/*
 * Core multiplication algorithm for the (modified) comb method.
 * This part is actually common with the basic comb method (GECC 3.44)
 *
 * Cost: d A + d D + 1 R
 *
 * For w=4,D=4S+4M,A=8M+3S,S=0.8M this gives about 1126*M, which with the
 * cost of ecp256_precompute_comb() gives ~2130*M + 2*I, which is ~170-270*M
 * better than the number for Jacobian cure shapes with 2*I in
 * "Analysis and optimization of elliptic-curve single-scalar multiplication",
 * by Bernstein & Lange, 2007.
 */
static int
ecp256_mul_comb_core(TlsEcpPoint *R, const unsigned char x[])
{
	const EcpXY *T = *this_cpu_ptr(&combT);
	TlsEcpPoint *Txi;
	size_t i = D;

	ttls_ecp_point_tmp_alloc_init(Txi, G_LIMBS, G_LIMBS, 0);
	ttls_mpi_alloc(&R->X, G_LIMBS * 2);
	ttls_mpi_alloc(&R->Y, G_LIMBS * 2);
	ttls_mpi_alloc(&R->Z, G_LIMBS + 1);

	/*
	 * We operate with precomputed table which is significantly smaller
	 * than L1d cache - for secp256 and w=7:
	 *
	 *	(sizeof(ECP)=(2 * 32)) * (1 << (w - 1)) = 4096
	 *
	 * Also there is no preemption and point doubling and addition
	 * aren't memory hungry, so once read T resides in L1d cache and
	 * we can address T directly without sacrificing safety against SCAs.
	 * FIXME in hyperthreading mode an aggressive sibling thread can evict
	 * the precomputed table.
	 *
	 * #TODO in the generic version, which uses much smaller W, the table
	 * is even smaller, so update the comment and probably use different
	 * approach.
	 *
	 * TODO #1064: 5. AVX2 - 4 points in parallel in OpenSSL,
	 * see ecp_nistz256_avx2_mul_g().
	 */
	ecp256_select_comb(R, T, W_SZ, x[i]);
	ttls_mpi_lset(&R->Z, 1);

	while (i--) {
		unsigned char ii = (x[i] & 0x7Fu) >> 1;

		/*
		 * TODO #1064 use merged doubling-addition formula.
		 *
		 * "New Composite Operations and Precomputation Schemefor
		 * Elliptic Curve Cryptosystems over Prime Fields" by Longa,
		 * 2008.
		 */

		MPI_CHK(ecp256_double_jac(R, R));

		Txi->X.s = 1;
		Txi->X.used = G_LIMBS;
		memcpy_fast(MPI_P(&Txi->X), &T[ii].x, G_LIMBS * CIL);
		Txi->Y.s = 1;
		Txi->Y.used = G_LIMBS;
		memcpy_fast(MPI_P(&Txi->Y), &T[ii].y, G_LIMBS * CIL);

		ecp256_safe_invert_jac(Txi, x[i] >> 7);

		MPI_CHK(ecp256_add_mixed(R, R, Txi));
	}

	return 0;
}

/*
 * Multiplication R = m * P using the comb method.
 *
 * It seems integer sub-decomposition, introduced in "Point Multiplication using
 * Integer Sub-Decompositionfor Elliptic Curve Cryptography" by Ajeena et all,
 * makes the computation 50% faster, but relatively complex transformation of
 * `m` is required.
 *
 * In order to prevent timing attacks, this function executes the exact same
 * sequence of (base field) operations for any valid m. It avoids any if-branch
 * or array index depending on the value of m.
 *
 * If @rng is true, the functions randomizes intermediate results in order to
 * prevent potential timing attacks targeting these results.
 * TODO #1064: double SCA protection?
 *
 * TODO #1064 normalize_jac uses inversions - it seems there are methods avoiding
 * inversions at all.
 *
 * TODO #1064: why wNAF isn't used? Is comb the most efficient method?
 * It seems WolfSSL's sp_256_ecc_mulmod_win_add_sub_avx2_4() also uses comb,
 * but with d=43 (w=6).
 * OpenSSL's ecp_nistz256_windowed_mul() use Booth windowed method.
 * It seems the both OpenSSL and WolfSSL don't use coordinates randomization.
 */
static int
ecp256_mul_comb(TlsEcpPoint *R, const TlsMpi *m, const unsigned long *P)
{
	/*
	 * Minimize the number of multiplications, that is minimize
	 * 10 * d * w + 18 * 2^(w-1) + 11 * d + 7 * w, with d = ceil(bits / w)
	 * (see costs of the various parts, with 1S = 1M).
	 * TODO #1064 make sure that w size is the best one.
	 */
	static const size_t COMB_MAX_D = G_BITS / 2 + 1;

	unsigned char m_is_odd;
	TlsMpi *M, *mm;
	unsigned char k[COMB_MAX_D];

	M = ttls_mpi_alloc_stack_init(G_LIMBS);
	mm = ttls_mpi_alloc_stack_init(G_LIMBS);

	BUILD_BUG_ON(D >= COMB_MAX_D);

	MPI_CHK(ecp256_precompute_comb(P));

	/*
	 * Make sure M is odd (M = m or M = N - m, since N is odd)
	 * using the fact that m * P = - (N - m) * P
	 */
	m_is_odd = (ttls_mpi_get_bit(m, 0) == 1);
	ttls_mpi_copy(M, m);
	ttls_mpi_sub_mpi(mm, &G.N, m);
	ecp256_safe_cond_assign(M, mm, !m_is_odd);

	/* Go for comb multiplication, R = M * P */
	ecp256_comb_fixed(k, D, W, M);
	MPI_CHK(ecp256_mul_comb_core(R, k));

	/* Now get m * P from M * P and normalize it. */
	ecp256_safe_invert_jac(R, !m_is_odd);
	MPI_CHK(ecp256_normalize_jac(R));

	return 0;
}

/**
 * Fixed-base comb method with V=d extended precomputed tables (GECC 3.45, 3.47).
 */
static int
ecp256_mul_comb_core_g(TlsEcpPoint *R, const unsigned char x[])
{
	TlsEcpPoint *Txi;
	size_t i = G_D;

	ttls_ecp_point_tmp_alloc_init(Txi, G_LIMBS, G_LIMBS, 0);
	ttls_mpi_alloc(&R->X, G_LIMBS * 2);
	ttls_mpi_alloc(&R->Y, G_LIMBS * 2);
	ttls_mpi_alloc(&R->Z, G_LIMBS + 1);

	/*
	 * Start with a non-zero point and randomize its coordinates.
	 *
	 * TODO #1064: now the talbe is more than 150KB, so need to implement
	 * constant time lookup.
	 *
	 * TODO #1064: 5. AVX2 - 4 points in parallel in OpenSSL,
	 * see ecp_nistz256_avx2_mul_g().
	 */
	ecp256_select_comb(R, combT_G[G_D], G_W_SZ, x[i]);
	ttls_mpi_lset(&R->Z, 1);

	while (i--) {
		unsigned char ii = (x[i] & 0x7Fu) >> 1;

		Txi->X.s = 1;
		Txi->X.used = G_LIMBS;
		memcpy_fast(MPI_P(&Txi->X), &combT_G[i][ii].x, G_LIMBS * CIL);
		Txi->Y.s = 1;
		Txi->Y.used = G_LIMBS;
		memcpy_fast(MPI_P(&Txi->Y), &combT_G[i][ii].y, G_LIMBS * CIL);

		ecp256_safe_invert_jac(Txi, x[i] >> 7);

		MPI_CHK(ecp256_add_mixed(R, R, Txi));
	}

	return 0;
}

/**
 * The ecp256_mul_comb() specialization for R = m * G.
 */
static int
ecp256_mul_comb_g(TlsEcpPoint *R, const TlsMpi *m)
{
	static const size_t COMB_MAX_D = G_BITS / 2 + 1;

	unsigned char m_is_odd;
	TlsMpi *M, *mm;
	unsigned char k[COMB_MAX_D];

	BUILD_BUG_ON(G_D >= COMB_MAX_D);

	M = ttls_mpi_alloc_stack_init(G_LIMBS);
	mm = ttls_mpi_alloc_stack_init(G_LIMBS);

	/*
	 * Make sure M is odd (M = m or M = N - m, since N is odd)
	 * using the fact that m * P = - (N - m) * P
	 */
	m_is_odd = (ttls_mpi_get_bit(m, 0) == 1);
	ttls_mpi_copy(M, m);
	ttls_mpi_sub_mpi(mm, &G.N, m);
	ecp256_safe_cond_assign(M, mm, !m_is_odd);

	/* Go for comb multiplication, R = M * G */
	ecp256_comb_fixed(k, G_D, G_W, M);
	MPI_CHK(ecp256_mul_comb_core_g(R, k));

	/* Now get m * G from M * G and normalize it. */
	ecp256_safe_invert_jac(R, !m_is_odd);
	MPI_CHK(ecp256_normalize_jac(R));

	return 0;
}

/*
 * Multiplication and addition of two points by integers: R = m * G + n * Q
 * In contrast to ttls_ecp_mul(), this function does not guarantee a constant
 * execution flow and timing - there is no secret data, so we don't need to care
 * about SCAs.
 *
 * TODO #769: The algorithm is naive. The Shamir's trick and/or
 * multi-exponentiation (Bodo MÃ¶ller, "Algorithms for multi-exponentiation")
 * should be used. See OpenSSL's ec_wNAF_mul() as the reference.
 */
static int
ecp256_muladd(TlsEcpPoint *R, const TlsMpi *m, const TlsEcpPoint *Q,
	      const TlsMpi *n)
{
	unsigned long pXY[G_LIMBS * 2];
	TlsEcpPoint *mP = ttls_mpool_alloc_stack(sizeof(TlsEcpPoint));
	ttls_ecp_point_init(mP);

	MPI_CHK(ecp256_mul_comb_g(mP, m));

	memcpy_fast(pXY, MPI_P(&Q->X), G_LIMBS * CIL);
	memcpy_fast(&pXY[G_LIMBS], MPI_P(&Q->Y), G_LIMBS * CIL);

	MPI_CHK(ecp256_mul_comb(R, n, pXY));

	MPI_CHK(ecp256_add_mixed(R, mP, R));
	MPI_CHK(ecp256_normalize_jac(R));

	return 0;
}

/**
 * Generate a keypair with configurable base point - SEC1 3.2.1:
 * generate d such that 1 <= n < N.
 */
int
ecp256_gen_keypair(TlsMpi *d, TlsEcpPoint *Q)
{
	int count = 0;

	/*
	 * Match the procedure given in RFC 6979 (deterministic ECDSA):
	 * - use the same byte ordering;
	 * - keep the leftmost bits bits of the generated octet string;
	 * - try until result is in the desired range.
	 * This also avoids any biais, which is especially important
	 * for ECDSA.
	 */
	do {
		ttls_mpi_fill_random(d, G_BITS / 8);

		/*
		 * Each try has at worst a probability 1/2 of failing
		 * (the msb has a probability 1/2 of being 0, and then
		 * the result will be < N), so after 30 tries failure
		 * probability is a most 2**(-30).
		 *
		 * For most curves, 1 try is enough with overwhelming
		 * probability, since N starts with a lot of 1s in
		 * binary, but some curves such as secp224k1 are
		 * actually very close to the worst case.
		 */
		if (WARN_ON_ONCE(++count > 10))
			return TTLS_ERR_ECP_RANDOM_FAILED;
	} while (ttls_mpi_eq_0(d) || ttls_mpi_cmp_mpi(d, &G.N) >= 0);

	return ecp256_mul_comb_g(Q, d);
}

/*
 * Derive a suitable integer for the group from a buffer of length len
 * SEC1 4.1.3 step 5 aka SEC1 4.1.4 step 3
 */
static void
derive_mpi(TlsMpi *x, const unsigned char *buf, size_t blen)
{
	const size_t n_size = G_BITS / 8;
	const size_t use_size = blen > n_size ? n_size : blen;

	ttls_mpi_read_binary(x, buf, use_size);

	/* While at it, reduce modulo N */
	if (ttls_mpi_cmp_mpi(x, &G.N) >= 0)
		ttls_mpi_sub_mpi(x, x, &G.N);
}

/**
 * This function computes the ECDSA signature of a hashed message (SEC1 4.1.3)
 * and writes it to a buffer, serialized as defined in RFC 8422 5.4.
 * Obviously, compared to SEC1 4.1.3, we skip step 4 (hash message).
 *
 * The sig buffer must be at least twice as large as the size of the curve used,
 * plus 9. For example, 73 Bytes if a 256-bit curve is used. A buffer length of
 * TTLS_ECDSA_MAX_LEN is always safe.
 *
 * If the bitlength of the message hash is larger than the bitlength of the
 * group order, then the hash is truncated as defined in Standards for Efficient
 * Cryptography Group (SECG): SEC1 Elliptic Curve Cryptography, section 4.1.3,
 * step 5.
 *
 * This is the late phase of ServerKeyExchange, so no need to clear the mpool
 * stack at the end of the function.
 */
static int
ecp256_ecdsa_sign(const TlsMpi *d, const unsigned char *hash, size_t hlen,
		  unsigned char *sig, size_t *slen)
{
	int key_tries, sign_tries, blind_tries, n;
	TlsMpi *k, *e, *t, *r, *s;
	TlsEcpPoint *R;

	n = max_t(size_t, G_LIMBS + d->used, hlen / CIL);
	k = ttls_mpi_alloc_stack_init(G_LIMBS * 2);
	e = ttls_mpi_alloc_stack_init(n * 2);
	t = ttls_mpi_alloc_stack_init(G_LIMBS);
	r = ttls_mpi_alloc_stack_init(G_LIMBS);
	s = ttls_mpi_alloc_stack_init(n * 2);
	R = ttls_mpool_alloc_stack(sizeof(*R));
	ttls_ecp_point_init(R);
	ttls_mpi_alloc(&R->Z, G_LIMBS * 2);

	sign_tries = 0;
	do {
		/* Generate a suitable ephemeral keypair and set r = xR mod n */
		key_tries = 0;
		do {
			MPI_CHK(ecp256_gen_keypair(k, R));
			ttls_mpi_mod_mpi(r, &R->X, &G.N);

			if (key_tries++ > 10)
				return TTLS_ERR_ECP_RANDOM_FAILED;
		} while (ttls_mpi_eq_0(r));

		/* Derive MPI from hashed message. */
		derive_mpi(e, hash, hlen);

		/*
		 * Generate a random value to blind inv_mod in next step,
		 * avoiding a potential timing leak.
		 */
		blind_tries = 0;
		do {
			ttls_mpi_fill_random(t, G_BITS / 8);

			/* See ttls_ecp_gen_keypair() */
			if (++blind_tries > 10)
				return TTLS_ERR_ECP_RANDOM_FAILED;
		} while (ttls_mpi_eq_0(t) || ttls_mpi_cmp_mpi(t, &G.N) >= 0);

		/* Compute s = (e + r * d) / k = t (e + rd) / (kt) mod n */
		ttls_mpi_mul_mpi(s, r, d);
		ttls_mpi_add_mpi(e, e, s);
		ttls_mpi_mul_mpi(e, e, t);
		ttls_mpi_mul_mpi(k, k, t);
		ttls_mpi_mod_mpi(k, k, &G.N);
		ecp256_inv_mod(s, k, &G.N);
		ttls_mpi_mul_mpi(s, s, e);
		ttls_mpi_mod_mpi(s, s, &G.N);

		if (sign_tries++ > 10)
			return TTLS_ERR_ECP_RANDOM_FAILED;
	} while (ttls_mpi_eq_0(s));

	return ecdsa_signature_to_asn1(r, s, sig, slen);
}

/*
 * Verify ECDSA signature of hashed message (SEC1 4.1.4)
 * Obviously, compared to SEC1 4.1.3, we skip step 2 (hash message).
 *
 * @buf		- the message hash;
 * @blen	- the length of the hash buf;
 * @Q		- the public key to use for verification;
 * @r		- the first integer of the signature;
 * @s		- the second integer of the signature.
 *
 * If the bitlength of the message hash is larger than the bitlength of the
 * group order, then the hash is truncated as defined in Standards for Efficient
 * Cryptography Group (SECG): SEC1 Elliptic Curve Cryptography, section 4.1.4,
 * step 3.
 */
static int
ecp256_ecdsa_verify(const unsigned char *buf, size_t blen, const TlsEcpPoint *Q,
		    const TlsMpi *r, const TlsMpi *s)
{
	TlsMpi *e, *s_inv, *u1, *u2;
	TlsEcpPoint *R;

	e = ttls_mpi_alloc_stack_init(G_LIMBS);
	s_inv = ttls_mpi_alloc_stack_init(G_LIMBS);
	u1 = ttls_mpi_alloc_stack_init(e->limbs + s_inv->limbs);
	u2 = ttls_mpi_alloc_stack_init(r->limbs + s_inv->limbs);
	R = ttls_mpool_alloc_stack(sizeof(*R));
	ttls_ecp_point_init(R);

	/* Step 1: make sure r and s are in range 1..n-1 */
	if (ttls_mpi_cmp_int(r, 1) < 0 || ttls_mpi_cmp_mpi(r, &G.N) >= 0
	    || ttls_mpi_cmp_int(s, 1) < 0 || ttls_mpi_cmp_mpi(s, &G.N) >= 0)
		return TTLS_ERR_ECP_VERIFY_FAILED;

	/* Step 3: derive MPI from hashed message. */
	derive_mpi(e, buf, blen);

	/* Step 4: u1 = e / s mod n, u2 = r / s mod n */
	ecp256_inv_mod(s_inv, s, &G.N);
	ttls_mpi_mul_mpi(u1, e, s_inv);
	ttls_mpi_mod_mpi(u1, u1, &G.N);
	ttls_mpi_mul_mpi(u2, r, s_inv);
	ttls_mpi_mod_mpi(u2, u2, &G.N);

	/*
	 * Step 5: R = u1 G + u2 Q
	 *
	 * Since we're not using any secret data, no need to pass a RNG to
	 * ttls_ecp_mul() for countermesures.
	 */
	MPI_CHK(ecp256_muladd(R, u1, Q, u2));
	if (ttls_ecp_is_zero(R))
		return TTLS_ERR_ECP_VERIFY_FAILED;

	/*
	 * Step 6: convert xR to an integer (no-op)
	 * Step 7: reduce xR mod n (gives v)
	 */
	ttls_mpi_mod_mpi(&R->X, &R->X, &G.N);

	/* Step 8: check if v (that is, R.X) is equal to r. */
	return ttls_mpi_cmp_mpi(&R->X, r);
}

const TlsEcpGrp SECP256_G ____cacheline_aligned = {
	.id		= TTLS_ECP_DP_SECP256R1,
	.bits		= G_BITS,

	.mul		= ecp256_mul_comb,
	.muladd		= ecp256_muladd,
	.gen_keypair	= ecp256_gen_keypair,
	.ecdsa_sign	= ecp256_ecdsa_sign,
	.ecdsa_verify	= ecp256_ecdsa_verify,
};
